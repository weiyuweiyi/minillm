torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2012 /home/bingxing2/home/scx7atk/work/minillm-zxc/train_minillm.py --base-path /home/bingxing2/home/scx7atk/work/minillm-zxc --model-path /home/bingxing2/public/models/llama2/Llama-2-7b-hf/ --teacher-model-path /home/bingxing2/public/models/llama2/Llama-2-13b-hf/ --ckpt-name llama2-7b --teacher-ckpt-name llama2-13b --n-gpu 4 --n-nodes 1 --teacher-model-fp16 --prompt-data-dir /home/bingxing2/home/scx7atk/work/minillm-zxc/processed_data/dolly/prompt/llama/ --lm-data-dir /home/bingxing2/home/scx7atk/work/minillm-zxc/processed_data/roberta/llama/512/20M/ --dev-num 1000 --num-workers 0 --epochs 10 --total-iters 5000 --kd-ratio 0.5 --batch-size 16 --lr 5e-6 --lr-min 5e-6 --gradient-accumulation-steps 1 --max-length 512 --max-prompt-length 256 --warmup-iters 100 --save /home/bingxing2/home/scx7atk/work/minillm-zxc/results/llama2/train/minillm-zxc/ --seed 10 --seed-ppo 42 --seed-lm 7 --save-interval 500 --eval-interval 100 --log-interval 16 --mid-log-num 1 --type minillm --ppo-epochs 4 --num-rollouts 64 --chunk-size 32 --length-norm --single-step-reg --teacher-mixed-alpha 0.2 --reward-scaling 0.5 --cliprange-reward 100 --do-sample --top-k 0 --top-p 1.0 --temperature 1.0 --deepspeed --deepspeed_config /home/bingxing2/home/scx7atk/work/minillm-zxc/configs/deepspeed/ds_config.json > /home/bingxing2/home/scx7atk/work/minillm-zxc/results/llama2/train/minillm-zxc//gpt2-xl-base-minillm-20240519-train.log 2>&1
PYTHONPATH=/home/bingxing2/home/scx7atk/work/minillm-zxc
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2024-05-19 21:28:30,910] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-19 21:28:30,910] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-19 21:28:30,910] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-19 21:28:30,910] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2024-05-19 21:28:42,180] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-05-19 21:28:42,180] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-05-19 21:28:42,180] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-05-19 21:28:42,180] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-05-19 21:28:42,180] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-05-19 21:28:42,180] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-05-19 21:28:42,180] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-05-19 21:28:42,180] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-05-19 21:28:42,180] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
arguments:
  model_path ................... /home/bingxing2/public/models/llama2/Llama-2-7b-hf/
  ckpt_name .................... llama2-7b
  n_gpu ........................ 4
  n_nodes ...................... 1
  model_type ................... gpt2
  teacher_model_type ........... None
  teacher_model_path ........... ['/home/bingxing2/public/models/llama2/Llama-2-13b-hf/']
  teacher_ckpt_name ............ llama2-13b
  teacher_model_fp16 ........... True
  model_parallel ............... False
  model_parallel_size .......... None
  no_value ..................... False
  dropout_path_rate ............ None
  fp32 ......................... False
  type ......................... minillm
  do_train ..................... False
  do_valid ..................... False
  do_eval ...................... False
  base_path .................... /home/bingxing2/home/scx7atk/work/minillm-zxc
  load ......................... None
  save ......................... /home/bingxing2/home/scx7atk/work/minillm-zxc/results/llama2/train/minillm-zxc/bs16-lr5e-06-G1-N4-NN1-lm1-len512/pe4_rs0.5_nr64_ln_sr_tm0.2
  log_interval ................. 16
  mid_log_num .................. 1
  save_interval ................ 500
  eval_interval ................ 100
  local_rank ................... 0
  save_additional_suffix ....... 
  save_rollout ................. False
  eb_sample_times .............. 3
  data_dir ..................... None
  processed_data_dir ........... None
  force_process ................ False
  force_process_demo ........... False
  data_process_workers ......... -1
  train_num .................... -1
  train_ratio .................. 1
  dev_num ...................... 1000
  dev_ratio .................... 1
  gen_num ...................... -1
  data_names ................... None
  prompt_type .................. None
  num_workers .................. 0
  max_prompt_length ............ 256
  min_prompt_length ............ 128
  json_data .................... False
  bin_data ..................... False
  txt_data ..................... False
  prompt_data_dir .............. /home/bingxing2/home/scx7atk/work/minillm-zxc/processed_data/dolly/prompt/llama/
  lm_data_dir .................. /home/bingxing2/home/scx7atk/work/minillm-zxc/processed_data/roberta/llama/512/20M/
  eval_ppl ..................... False
  eval_rw ...................... False
  eval_gen ..................... False
  only_prompt .................. False
  batch_size ................... 16
  eval_batch_size .............. 32
  clip_grad .................... 1.0
  total_iters .................. 5000
  train_iters_per_epoch ........ -1
  max_length ................... 512
  seed ......................... 10
  seed_order ................... 42
  seed_data .................... 42
  seed_ppo ..................... 42
  seed_lm ...................... 7
  epochs ....................... 10
  training_epochs .............. 10000
  gradient_accumulation_steps .. 1
  gradient_checkpointing ....... False
  attn_dtype ................... None
  lr ........................... 5e-06
  lr_min ....................... 5e-06
  weight_decay ................. 0.01
  loss_scale ................... 65536
  kd_ratio ..................... 0.5
  warmup_iters ................. 100
  lr_decay_iters ............... None
  lr_decay_style ............... noam
  scheduler_name ............... constant_trm
  reward_scaling ............... 0.5
  cliprange_reward ............. 100.0
  ppo_epochs ................... 4
  num_rollouts ................. 64
  num_rollouts_per_device ...... 16
  cliprange .................... 0.2
  chunk_size ................... 32
  gamma ........................ 0.95
  length_norm .................. True
  single_step_reg .............. True
  teacher_mixed_alpha .......... 0.2
  lm_coef ...................... 1
  top_k ........................ 0
  top_p ........................ 1.0
  do_sample .................... True
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  num_beams .................... 1
  temperature .................. 1.0
  peft ......................... None
  peft_lora_r .................. 8
  peft_lora_alpha .............. 32
  peft_lora_dropout ............ 0.1
  peft_name .................... None
  peft_path .................... None
  teacher_peft_name ............ None
  teacher_peft_path ............ None
  deepspeed .................... True
  deepspeed_config ............. /home/bingxing2/home/scx7atk/work/minillm-zxc/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  rank ......................... 0
  world_size ................... 4
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:06<00:12,  6.03s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:06<00:12,  6.05s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:06<00:12,  6.10s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:06<00:12,  6.10s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:11<00:05,  5.61s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:11<00:05,  5.62s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:11<00:05,  5.60s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:11<00:05,  5.63s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:14<00:00,  4.68s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:14<00:00,  4.97s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:14<00:00,  4.68s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:14<00:00,  4.98s/it]
/home/bingxing2/home/scx7atk/work/minillm/transformers/src/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/bingxing2/home/scx7atk/work/minillm/transformers/src/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/bingxing2/home/scx7atk/work/minillm/transformers/src/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/bingxing2/home/scx7atk/work/minillm/transformers/src/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
Loading checkpoint shards: 100%|██████████| 3/3 [00:14<00:00,  4.67s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:14<00:00,  4.97s/it]
/home/bingxing2/home/scx7atk/work/minillm/transformers/src/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/bingxing2/home/scx7atk/work/minillm/transformers/src/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
Loading checkpoint shards: 100%|██████████| 3/3 [00:14<00:00,  4.69s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:14<00:00,  4.99s/it]
/home/bingxing2/home/scx7atk/work/minillm/transformers/src/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/bingxing2/home/scx7atk/work/minillm/transformers/src/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
 > number of parameters: 13015864320
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:13<00:13, 13.17s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:13<00:13, 13.16s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:13<00:13, 13.19s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:16<00:16, 16.47s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.23s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.97s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.25s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.99s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.25s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.99s/it]
 > number of parameters: 6738415616
Model load time: 18.423923015594482s
 > number of parameters: 6738M
[2024-05-19 21:29:17,553] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|██████████| 2/2 [00:21<00:00,  9.81s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:21<00:00, 10.81s/it]
[2024-05-19 21:29:21,797] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-05-19 21:29:21,800] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2024-05-19 21:29:21,800] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-05-19 21:29:21,824] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2024-05-19 21:29:21,824] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2024-05-19 21:29:21,824] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 1 optimizer
[2024-05-19 21:29:21,824] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500,000,000
[2024-05-19 21:29:21,824] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500,000,000
[2024-05-19 21:29:21,824] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False
[2024-05-19 21:29:21,824] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Traceback (most recent call last):
  File "/home/bingxing2/home/scx7atk/work/minillm-zxc/train_minillm.py", line 131, in <module>
    main()
  File "/home/bingxing2/home/scx7atk/work/minillm-zxc/train_minillm.py", line 116, in main
    train(
  File "/home/bingxing2/home/scx7atk/work/minillm-zxc/minillm/__init__.py", line 22, in train
    trainer = PPOTrainer(
  File "/home/bingxing2/home/scx7atk/work/minillm-zxc/minillm/trainer.py", line 87, in __init__
    self.model, self.opt, self.scheduler = self.setup_ds(self.model, self.opt, self.scheduler)
  File "/home/bingxing2/home/scx7atk/work/minillm-zxc/minillm/trainer.py", line 145, in setup_ds
    model, optimizer, _, scheduler = deepspeed.initialize(
  File "/home/bingxing2/home/scx7atk/.conda/envs/minillm/lib/python3.10/site-packages/deepspeed/__init__.py", line 171, in initialize
    engine = DeepSpeedEngine(args=args,
  File "/home/bingxing2/home/scx7atk/.conda/envs/minillm/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 310, in __init__
    self._configure_optimizer(optimizer, model_parameters)
  File "/home/bingxing2/home/scx7atk/.conda/envs/minillm/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1209, in _configure_optimizer
    self.optimizer = self._configure_zero_optimizer(basic_optimizer)
  File "/home/bingxing2/home/scx7atk/.conda/envs/minillm/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1444, in _configure_zero_optimizer
    optimizer = DeepSpeedZeroOptimizer(
  File "/home/bingxing2/home/scx7atk/.conda/envs/minillm/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 346, in __init__
    self.device).clone().float().detach())
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.14 GiB (GPU 3; 39.39 GiB total capacity; 36.98 GiB already allocated; 1.84 GiB free; 36.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "/home/bingxing2/home/scx7atk/work/minillm-zxc/train_minillm.py", line 131, in <module>
    main()
  File "/home/bingxing2/home/scx7atk/work/minillm-zxc/train_minillm.py", line 116, in main
    train(
  File "/home/bingxing2/home/scx7atk/work/minillm-zxc/minillm/__init__.py", line 22, in train
    trainer = PPOTrainer(
  File "/home/bingxing2/home/scx7atk/work/minillm-zxc/minillm/trainer.py", line 87, in __init__
    self.model, self.opt, self.scheduler = self.setup_ds(self.model, self.opt, self.scheduler)
  File "/home/bingxing2/home/scx7atk/work/minillm-zxc/minillm/trainer.py", line 145, in setup_ds
    model, optimizer, _, scheduler = deepspeed.initialize(
  File "/home/bingxing2/home/scx7atk/.conda/envs/minillm/lib/python3.10/site-packages/deepspeed/__init__.py", line 171, in initialize
    engine = DeepSpeedEngine(args=args,
  File "/home/bingxing2/home/scx7atk/.conda/envs/minillm/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 310, in __init__
    self._configure_optimizer(optimizer, model_parameters)
  File "/home/bingxing2/home/scx7atk/.conda/envs/minillm/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1209, in _configure_optimizer
    self.optimizer = self._configure_zero_optimizer(basic_optimizer)
  File "/home/bingxing2/home/scx7atk/.conda/envs/minillm/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1444, in _configure_zero_optimizer
    optimizer = DeepSpeedZeroOptimizer(
  File "/home/bingxing2/home/scx7atk/.conda/envs/minillm/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 346, in __init__
    self.device).clone().float().detach())
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.14 GiB (GPU 1; 39.39 GiB total capacity; 36.98 GiB already allocated; 1.84 GiB free; 36.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "/home/bingxing2/home/scx7atk/work/minillm-zxc/train_minillm.py", line 131, in <module>
    main()
  File "/home/bingxing2/home/scx7atk/work/minillm-zxc/train_minillm.py", line 116, in main
    train(
  File "/home/bingxing2/home/scx7atk/work/minillm-zxc/minillm/__init__.py", line 22, in train
    trainer = PPOTrainer(
  File "/home/bingxing2/home/scx7atk/work/minillm-zxc/minillm/trainer.py", line 87, in __init__
    self.model, self.opt, self.scheduler = self.setup_ds(self.model, self.opt, self.scheduler)
  File "/home/bingxing2/home/scx7atk/work/minillm-zxc/minillm/trainer.py", line 145, in setup_ds
    model, optimizer, _, scheduler = deepspeed.initialize(
  File "/home/bingxing2/home/scx7atk/.conda/envs/minillm/lib/python3.10/site-packages/deepspeed/__init__.py", line 171, in initialize
    engine = DeepSpeedEngine(args=args,
  File "/home/bingxing2/home/scx7atk/.conda/envs/minillm/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 310, in __init__
    self._configure_optimizer(optimizer, model_parameters)
  File "/home/bingxing2/home/scx7atk/.conda/envs/minillm/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1209, in _configure_optimizer
    self.optimizer = self._configure_zero_optimizer(basic_optimizer)
  File "/home/bingxing2/home/scx7atk/.conda/envs/minillm/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1444, in _configure_zero_optimizer
    optimizer = DeepSpeedZeroOptimizer(
  File "/home/bingxing2/home/scx7atk/.conda/envs/minillm/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 346, in __init__
    self.device).clone().float().detach())
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.14 GiB (GPU 2; 39.39 GiB total capacity; 36.98 GiB already allocated; 1.84 GiB free; 36.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "/home/bingxing2/home/scx7atk/work/minillm-zxc/train_minillm.py", line 131, in <module>
    main()
  File "/home/bingxing2/home/scx7atk/work/minillm-zxc/train_minillm.py", line 116, in main
    train(
  File "/home/bingxing2/home/scx7atk/work/minillm-zxc/minillm/__init__.py", line 22, in train
    trainer = PPOTrainer(
  File "/home/bingxing2/home/scx7atk/work/minillm-zxc/minillm/trainer.py", line 87, in __init__
    self.model, self.opt, self.scheduler = self.setup_ds(self.model, self.opt, self.scheduler)
  File "/home/bingxing2/home/scx7atk/work/minillm-zxc/minillm/trainer.py", line 145, in setup_ds
    model, optimizer, _, scheduler = deepspeed.initialize(
  File "/home/bingxing2/home/scx7atk/.conda/envs/minillm/lib/python3.10/site-packages/deepspeed/__init__.py", line 171, in initialize
    engine = DeepSpeedEngine(args=args,
  File "/home/bingxing2/home/scx7atk/.conda/envs/minillm/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 310, in __init__
    self._configure_optimizer(optimizer, model_parameters)
  File "/home/bingxing2/home/scx7atk/.conda/envs/minillm/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1209, in _configure_optimizer
    self.optimizer = self._configure_zero_optimizer(basic_optimizer)
  File "/home/bingxing2/home/scx7atk/.conda/envs/minillm/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1444, in _configure_zero_optimizer
    optimizer = DeepSpeedZeroOptimizer(
  File "/home/bingxing2/home/scx7atk/.conda/envs/minillm/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 346, in __init__
    self.device).clone().float().detach())
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.14 GiB (GPU 0; 39.39 GiB total capacity; 36.98 GiB already allocated; 1.84 GiB free; 36.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 674825) of binary: /home/bingxing2/home/scx7atk/.conda/envs/minillm/bin/python
Traceback (most recent call last):
  File "/home/bingxing2/home/scx7atk/.conda/envs/minillm/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/bingxing2/home/scx7atk/.conda/envs/minillm/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/home/bingxing2/home/scx7atk/.conda/envs/minillm/lib/python3.10/site-packages/torch/distributed/run.py", line 762, in main
    run(args)
  File "/home/bingxing2/home/scx7atk/.conda/envs/minillm/lib/python3.10/site-packages/torch/distributed/run.py", line 753, in run
    elastic_launch(
  File "/home/bingxing2/home/scx7atk/.conda/envs/minillm/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/bingxing2/home/scx7atk/.conda/envs/minillm/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 246, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/bingxing2/home/scx7atk/work/minillm-zxc/train_minillm.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-05-19_21:29:34
  host      : paraai-n32-h-01-agent-85.paraai-n32-h-01.com
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 674826)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-05-19_21:29:34
  host      : paraai-n32-h-01-agent-85.paraai-n32-h-01.com
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 674827)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-05-19_21:29:34
  host      : paraai-n32-h-01-agent-85.paraai-n32-h-01.com
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 674828)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-05-19_21:29:34
  host      : paraai-n32-h-01-agent-85.paraai-n32-h-01.com
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 674825)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
