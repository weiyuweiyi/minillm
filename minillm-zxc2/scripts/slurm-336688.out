torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2012 /home/bingxing2/home/scx7atk/work/minillm-zxc/train_minillm.py --base-path /home/bingxing2/home/scx7atk/work/minillm-zxc --model-path /home/bingxing2/home/scx7atk/pt_models/llama2/Llama-2-7b-hf/ --teacher-model-path /home/bingxing2/home/scx7atk/pt_models/llama2/Llama-2-13b-hf/ --ckpt-name llama2-7b --teacher-ckpt-name llama2-13b --n-gpu 4 --n-nodes 1 --teacher-model-fp16 --model-parallel --model-parallel-size 8 --prompt-data-dir /home/bingxing2/home/scx7atk/work/minillm-zxc/processed_data/dolly/prompt/llama/ --lm-data-dir /home/bingxing2/home/scx7atk/work/minillm-zxc/processed_data/roberta/llama/512/20M/ --dev-num 1000 --num-workers 0 --epochs 10 --total-iters 5000 --kd-ratio 0.5 --batch-size 16 --lr 5e-6 --lr-min 5e-6 --gradient-accumulation-steps 1 --max-length 512 --max-prompt-length 256 --warmup-iters 100 --save /home/bingxing2/home/scx7atk/work/minillm-zxc/results/llama2/train/minillm-zxc/ --seed 10 --seed-ppo 42 --seed-lm 7 --save-interval 500 --eval-interval 100 --log-interval 16 --mid-log-num 1 --type minillm --ppo-epochs 4 --num-rollouts 64 --chunk-size 32 --length-norm --single-step-reg --teacher-mixed-alpha 0.2 --reward-scaling 0.5 --cliprange-reward 100 --do-sample --top-k 0 --top-p 1.0 --temperature 1.0 --deepspeed --deepspeed_config /home/bingxing2/home/scx7atk/work/minillm-zxc/configs/deepspeed/ds_config_zero2_offload.json > /home/bingxing2/home/scx7atk/work/minillm-zxc/results/llama2/train/minillm-zxc//gpt2-xl-base-minillm-20240521-train.log 2>&1
PYTHONPATH=/home/bingxing2/home/scx7atk/work/minillm-zxc
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2024-05-21 18:39:29,190] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-21 18:39:29,190] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-21 18:39:29,196] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-21 18:39:29,208] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-21 18:39:44,007] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
using world size: 4[2024-05-21 18:39:44,007] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-05-21 18:39:44,007] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-05-21 18:39:44,007] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented

[2024-05-21 18:39:44,007] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-05-21 18:39:44,007] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-05-21 18:39:44,008] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-05-21 18:39:44,008] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-05-21 18:39:44,008] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
Traceback (most recent call last):
  File "/home/bingxing2/home/scx7atk/work/minillm-zxc/train_minillm.py", line 131, in <module>
    main()
  File "/home/bingxing2/home/scx7atk/work/minillm-zxc/train_minillm.py", line 79, in main
    initialize(args)
  File "/home/bingxing2/home/scx7atk/work/minillm-zxc/utils.py", line 128, in initialize
    assert dist.get_world_size() % args.model_parallel_size == 0 
AssertionError
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/bingxing2/home/scx7atk/work/minillm-zxc/train_minillm.py", line 131, in <module>
  File "/home/bingxing2/home/scx7atk/work/minillm-zxc/train_minillm.py", line 131, in <module>
Traceback (most recent call last):
  File "/home/bingxing2/home/scx7atk/work/minillm-zxc/train_minillm.py", line 131, in <module>
        main()main()

  File "/home/bingxing2/home/scx7atk/work/minillm-zxc/train_minillm.py", line 79, in main
  File "/home/bingxing2/home/scx7atk/work/minillm-zxc/train_minillm.py", line 79, in main
    main()
  File "/home/bingxing2/home/scx7atk/work/minillm-zxc/train_minillm.py", line 79, in main
        initialize(args)initialize(args)

  File "/home/bingxing2/home/scx7atk/work/minillm-zxc/utils.py", line 128, in initialize
  File "/home/bingxing2/home/scx7atk/work/minillm-zxc/utils.py", line 128, in initialize
    initialize(args)
  File "/home/bingxing2/home/scx7atk/work/minillm-zxc/utils.py", line 128, in initialize
            assert dist.get_world_size() % args.model_parallel_size == 0 assert dist.get_world_size() % args.model_parallel_size == 0 assert dist.get_world_size() % args.model_parallel_size == 0 


AssertionErrorAssertionErrorAssertionError


WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1327323 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 1327321) of binary: /home/bingxing2/home/scx7atk/.conda/envs/minillm/bin/python
Traceback (most recent call last):
  File "/home/bingxing2/home/scx7atk/.conda/envs/minillm/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/bingxing2/home/scx7atk/.conda/envs/minillm/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/home/bingxing2/home/scx7atk/.conda/envs/minillm/lib/python3.10/site-packages/torch/distributed/run.py", line 762, in main
    run(args)
  File "/home/bingxing2/home/scx7atk/.conda/envs/minillm/lib/python3.10/site-packages/torch/distributed/run.py", line 753, in run
    elastic_launch(
  File "/home/bingxing2/home/scx7atk/.conda/envs/minillm/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/bingxing2/home/scx7atk/.conda/envs/minillm/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 246, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/bingxing2/home/scx7atk/work/minillm-zxc/train_minillm.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-05-21_18:39:45
  host      : paraai-n32-h-01-agent-155.paraai-n32-h-01.com
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 1327322)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-05-21_18:39:45
  host      : paraai-n32-h-01-agent-155.paraai-n32-h-01.com
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 1327324)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-05-21_18:39:45
  host      : paraai-n32-h-01-agent-155.paraai-n32-h-01.com
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 1327321)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
